{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2YPfEGdWOjh"
      },
      "source": [
        "## Dataset Download\n",
        "\n",
        "First, we download all post with tag 'nlp' and we fetch it in JSON format. In this retrieval, we also pull the accepted answer id to indicate later if a post has been answered or not\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wC9j5J5fjMh",
        "outputId": "d0406582-6bb9-47ce-a3e4-11bc6014eb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgvBLuEXBEQj",
        "outputId": "660a6048-f38c-4653-a39d-cdbc41d5cbc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Fetching page 6...\n",
            "Fetching page 7...\n",
            "Fetching page 8...\n",
            "Fetching page 9...\n",
            "Fetching page 10...\n",
            "Fetching page 11...\n",
            "Fetching page 12...\n",
            "Fetching page 13...\n",
            "Fetching page 14...\n",
            "Fetching page 15...\n",
            "Fetching page 16...\n",
            "Fetching page 17...\n",
            "Fetching page 18...\n",
            "Fetching page 19...\n",
            "Fetching page 20...\n",
            "Fetching page 21...\n",
            "Fetching page 22...\n",
            "Fetching page 23...\n",
            "Fetching page 24...\n",
            "Fetching page 25...\n",
            "Fetching page 26...\n",
            "Fetching page 27...\n",
            "Fetching page 28...\n",
            "Fetching page 29...\n",
            "Fetching page 30...\n",
            "Fetching page 31...\n",
            "Fetching page 32...\n",
            "Fetching page 33...\n",
            "Fetching page 34...\n",
            "Fetching page 35...\n",
            "Fetching page 36...\n",
            "Fetching page 37...\n",
            "Fetching page 38...\n",
            "Fetching page 39...\n",
            "Fetching page 40...\n",
            "Fetching page 41...\n",
            "Fetching page 42...\n",
            "Fetching page 43...\n",
            "Fetching page 44...\n",
            "Fetching page 45...\n",
            "Fetching page 46...\n",
            "Fetching page 47...\n",
            "Fetching page 48...\n",
            "Fetching page 49...\n",
            "Fetching page 50...\n",
            "Fetching page 51...\n",
            "Fetching page 52...\n",
            "Fetching page 53...\n",
            "Fetching page 54...\n",
            "Fetching page 55...\n",
            "Fetching page 56...\n",
            "Fetching page 57...\n",
            "Fetching page 58...\n",
            "Fetching page 59...\n",
            "Fetching page 60...\n",
            "Fetching page 61...\n",
            "Fetching page 62...\n",
            "Fetching page 63...\n",
            "Fetching page 64...\n",
            "Fetching page 65...\n",
            "Fetching page 66...\n",
            "Fetching page 67...\n",
            "Fetching page 68...\n",
            "Fetching page 69...\n",
            "Fetching page 70...\n",
            "Fetching page 71...\n",
            "Fetching page 72...\n",
            "Fetching page 73...\n",
            "Fetching page 74...\n",
            "Fetching page 75...\n",
            "Fetching page 76...\n",
            "Fetching page 77...\n",
            "Fetching page 78...\n",
            "Fetching page 79...\n",
            "Fetching page 80...\n",
            "Fetching page 81...\n",
            "Fetching page 82...\n",
            "Fetching page 83...\n",
            "Fetching page 84...\n",
            "Fetching page 85...\n",
            "Fetching page 86...\n",
            "Fetching page 87...\n",
            "Fetching page 88...\n",
            "Fetching page 89...\n",
            "Fetching page 90...\n",
            "Fetching page 91...\n",
            "Fetching page 92...\n",
            "Fetching page 93...\n",
            "Fetching page 94...\n",
            "Fetching page 95...\n",
            "Fetching page 96...\n",
            "Fetching page 97...\n",
            "Fetching page 98...\n",
            "Fetching page 99...\n",
            "Fetching page 100...\n",
            "Fetching page 101...\n",
            "Fetching page 102...\n",
            "Fetching page 103...\n",
            "Fetching page 104...\n",
            "Fetching page 105...\n",
            "Fetching page 106...\n",
            "Fetching page 107...\n",
            "Fetching page 108...\n",
            "Fetching page 109...\n",
            "Fetching page 110...\n",
            "Fetching page 111...\n",
            "Fetching page 112...\n",
            "Fetching page 113...\n",
            "Fetching page 114...\n",
            "Fetching page 115...\n",
            "Fetching page 116...\n",
            "Fetching page 117...\n",
            "Fetching page 118...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Backoff detected! Sleeping for 10 seconds...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "No more pages. Switching to time-based pagination...\n",
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "\n",
            "Total questions fetched: 20100\n",
            "Data saved to nlp_questions_only.json ✅\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to clean HTML tags and remove newline characters\n",
        "def clean_html(body):\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(body, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Remove newline characters and extra spaces\n",
        "    return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "# Base API URL\n",
        "BASE_URL = \"https://api.stackexchange.com/2.3/questions\"\n",
        "\n",
        "# API Parameters\n",
        "params = {\n",
        "    \"order\": \"desc\",\n",
        "    \"sort\": \"creation\",\n",
        "    \"tagged\": \"nlp\",\n",
        "    \"site\": \"stackoverflow\",\n",
        "    \"filter\": \"withbody\",\n",
        "    \"pagesize\": 100,  # Max per request\n",
        "    \"page\": 1,  # Start from page 1\n",
        "    \"key\": \"#######################\" ## API Key from Stack Exchange\n",
        "}\n",
        "\n",
        "# Variables to store results\n",
        "questions = []\n",
        "total_needed = 20100  # Target number of questions\n",
        "\n",
        "while len(questions) < total_needed:\n",
        "    print(f\"Fetching page {params['page']}...\")\n",
        "\n",
        "    # Make API Request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    # Handle API errors\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code} - {response.text}\")\n",
        "        break\n",
        "\n",
        "    # Handle backoff if present\n",
        "    if \"backoff\" in data:\n",
        "        backoff_time = data[\"backoff\"]\n",
        "        print(f\"Backoff detected! Sleeping for {backoff_time} seconds...\")\n",
        "        time.sleep(backoff_time)\n",
        "\n",
        "    # Extract required fields\n",
        "    items = data.get(\"items\", [])\n",
        "    for item in items:\n",
        "        question = {\n",
        "            \"tags\": item.get(\"tags\", []),\n",
        "            \"title\": item.get(\"title\", \"\"),\n",
        "            \"body\": clean_html(item.get(\"body\", \"\")),  # Clean HTML tags and remove \\n\n",
        "            \"creation_date\": datetime.utcfromtimestamp(item.get(\"creation_date\", 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            \"view_count\": item.get(\"view_count\", 0),\n",
        "            \"score\": item.get(\"score\", 0),\n",
        "            \"accepted_answer_id\": item.get(\"accepted_answer_id\", None)  # Add accepted_answer_id\n",
        "        }\n",
        "        questions.append(question)\n",
        "\n",
        "    # Stop if there are no more pages\n",
        "    if not data.get(\"has_more\", False):\n",
        "        print(\"No more pages. Switching to time-based pagination...\")\n",
        "        last_question_date = items[-1][\"creation_date\"] if items else params.get(\"todate\")\n",
        "        if last_question_date:\n",
        "            params[\"todate\"] = last_question_date - 1\n",
        "            params[\"page\"] = 1\n",
        "        else:\n",
        "            break\n",
        "    else:\n",
        "        # Increase page number for next request\n",
        "        params[\"page\"] += 1\n",
        "\n",
        "    # Respect API rate limits\n",
        "    time.sleep(1)  # Avoid hitting API quota too fast\n",
        "\n",
        "# Trim excess questions if needed\n",
        "questions = questions[:total_needed]\n",
        "\n",
        "# Print Results Summary\n",
        "print(f\"\\nTotal questions fetched: {len(questions)}\")\n",
        "\n",
        "# Save results to a JSON file\n",
        "with open(\"/content/drive/MyDrive/ANLP_A2/nlp_questions_only.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(questions, f, indent=2)\n",
        "\n",
        "print(\"Data saved to nlp_questions_only.json ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are total of 20100 questions post downloaded along with its metadata such as tags, title, creation date, view count, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8p1_kk04W6nh"
      },
      "source": [
        "After we pull the posts with tag 'nlp', we filter post with accepted answer only, as in this study we will use the combination of title, body, and answer for document classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVoj6deqBESt",
        "outputId": "2f70679c-f8c3-4295-8199-559110cff977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of output_file: 8165\n"
          ]
        }
      ],
      "source": [
        "# Function to process and filter questions with accepted answers\n",
        "def filter_questions_with_accepted_answers(input_file, output_file):\n",
        "    with open(input_file, 'r') as file:\n",
        "        questions = json.load(file)\n",
        "\n",
        "    filtered_questions = []\n",
        "\n",
        "    for question in questions:\n",
        "        # Check if the question has an accepted answer by looking for a non-null 'accepted_answer_id'\n",
        "        if question.get('accepted_answer_id') is not None:\n",
        "            # Add the question to the filtered list if it has an accepted answer\n",
        "            filtered_questions.append(question)\n",
        "\n",
        "    # Save the filtered questions with accepted answers to a new file\n",
        "    with open(output_file, 'w') as output:\n",
        "        json.dump(filtered_questions, output, indent=4)\n",
        "\n",
        "# Example usage\n",
        "input_file = '/content/drive/MyDrive/ANLP_A2/nlp_questions_only.json'  # Path to your input JSON file\n",
        "output_file = '/content/drive/MyDrive/ANLP_A2/nlp_questions_with_accepted_answers.json'  # Path to output file\n",
        "filter_questions_with_accepted_answers(input_file, output_file)\n",
        "\n",
        "# Check the length of output_file\n",
        "with open(output_file, 'r') as output:\n",
        "    output_data = json.load(output)\n",
        "    print(f\"Length of output_file: {len(output_data)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbLsla9oXdVW"
      },
      "source": [
        "Finally, with the post with answer file, we request the answer of each post one by one using stack exchange API. There are a total of 8165 questions with answer retrieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp1biX_NBEU9"
      },
      "outputs": [],
      "source": [
        "# Function to clean HTML tags and remove newline characters\n",
        "def clean_html(body):\n",
        "    # Remove HTML tags using BeautifulSoup\n",
        "    soup = BeautifulSoup(body, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Remove newline characters and extra spaces\n",
        "    return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "# Function to fetch accepted answer for a given answer_id\n",
        "def fetch_answer(answer_id, api_key):\n",
        "    url = f\"https://api.stackexchange.com/2.3/answers/{answer_id}?order=desc&sort=activity&site=stackoverflow&key={api_key}&filter=withbody\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "\n",
        "        # Check if the response contains items (answers)\n",
        "        if 'items' in data and data['items']:\n",
        "            answer = data['items'][0]\n",
        "\n",
        "\n",
        "            # Check if the body exists in the answer\n",
        "            if 'body' in answer:\n",
        "                cleaned_answer = clean_html(answer['body'])\n",
        "                votes = answer.get('score', 0)  # Get the vote count, default to 0 if not found\n",
        "                print('Done')\n",
        "                return cleaned_answer, votes\n",
        "            else:\n",
        "                print(f\"Answer body not found for answer_id {answer_id}\")\n",
        "                return None, None\n",
        "        else:\n",
        "            print(f\"No items found for answer_id {answer_id}\")\n",
        "            return None, None\n",
        "    else:\n",
        "        print(f\"Error fetching answer {answer_id}: {response.status_code}\")\n",
        "        return None, None\n",
        "\n",
        "# Load JSON data from file\n",
        "with open(\"/content/drive/MyDrive/ANLP_A2/nlp_questions_with_accepted_answers.json\", \"r\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# API key (replace with your actual API key)\n",
        "api_key = \"rl_WQFnNLX6hFZoyWkU77T9tFiNg\"\n",
        "\n",
        "# Add accepted answer and vote count to each question\n",
        "for question in data:\n",
        "    accepted_answer, votes = fetch_answer(question['accepted_answer_id'], api_key)\n",
        "\n",
        "    # If we fetched an accepted answer, add it to the question\n",
        "    if accepted_answer:\n",
        "        question['accepted_answer'] = accepted_answer\n",
        "        question['accepted_answer_votes'] = votes\n",
        "\n",
        "    # Sleep for 1 second to avoid hitting API quota too fast\n",
        "    time.sleep(1)\n",
        "\n",
        "# Output the updated JSON with accepted answers and vote counts\n",
        "with open(\"questions_and_answers.json\", \"w\") as file:\n",
        "    json.dump(data, file, indent=4)\n",
        "\n",
        "# Print the updated JSON to verify\n",
        "#print(json.dumps(data, indent=4))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
